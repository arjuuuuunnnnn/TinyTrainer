# TinyTrainer

This pipeline demonstrates a systematic approach to fine-tuning small language models for enhanced reasoning and instruction-following capabilities.

## Project Overview

This project implements a multi-stage training approach to enhance TinyLlama's reasoning and instruction-following capabilities through:

1. **Dataset Curation & Formatting** - Custom preprocessing pipelines for diverse datasets
2. **Supervised Fine-tuning (SFT)** - Multi-domain instruction following training
3. **Chain-of-Thought (CoT) Training** - Enhanced reasoning capability development
4. **Preference Data Generation** - Collecting multi-response data for alignment(planned)
4. **DPO Training** - Direct Preference Optimization for model alignment(planned)

## Training Pipeline Status

- [x] Dataset Curation & Formatting
- [x] Supervised Fine-tuning (SFT)
- [x] Chain-of-Thought (CoT) Training
- [ ] Preference Data Generation
- [ ] DPO Training

## Dataset Curation & Formatting
This stage involves custom preprocessing pipelines for various datasets

Please read the [Dataset Curation & Formatting README](./preprocess/README.md) for more details.

Get started with dataset curation by running:

